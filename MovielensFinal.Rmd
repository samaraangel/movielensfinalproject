---
title: "MovieLens Final Project"
subtitle: "Harvard EdX Professional Data Science Certificate" 
author: "Samara Angel"
date: "Spring 2021"
output: 
  pdf_document
header-includes:
- \usepackage{float} 
- \floatplacement{figure}{H}
geometry: margin = 2cm
urlcolor: blue
---
```{r setup, include=FALSE}
library(knitr)
#library(ggplot2)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::opts_chunk$set(error = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H', out.extra = '')
```

# Introduction
Recommendation systems are made using machine learning algorithms. In the case of movie recommendations, they are made with the goal of predicting the rating a user would make of a movie in order to recommend movies to that user. In October of 2006, Netflix set a challenge to data scientists to improve the recommendation system of their in-house software, Cinematch. The winning team who could improve the algorithm by 10% would receive one million dollars. The winner of the challenge was announced in September of 2009 and was a group called BellKor’s Pragmatic Chaos, a 7-person team of statisticians, computer-engineers, and machine learning experts. While the Netflix data is not available to the public, a similar database generated by the GroupLens research lab is available. The database contains over 20 million ratings, over 27,000 movies, and over 138,000 users. In this project, I will use a subset of the GroupLens data, available in the dslabs package. Each row of the data is representative of one user’s rating of one specific movie, but it is important to note that not every user rated each movie. A recommendation system essentially fills in predicted ratings for those that do not yet exist. I will use Root Mean Square Error (RMSE) as the evaluation for the models I develop in this project, which is the same metric used to evaluate the Netflix Challenge. The RMSE is the square root of the averaged squared difference between the target value and the value predicted by the model. My goal in this project is to minimize RMSE, specifically to get a final RMSE < 0.86490. To do this, I will pre-process the data to generate a train set, “edx,” (90%) and a test set, “validation,” (10%). I will then split the EdX set into its own train set, “edx_train_set,” (90%) and test set, “edx_test_set,” (10%), and will use these data in order to train the models which I develop. Once I have trained each model using the edx_train_set and edx_test_set, I will then use the Validation set (the final hold-out set) to test my recommendation system. 

The 10M version of the MovieLens dataset is available at the following link: <https://grouplens.org/datasets/movielens/10m/>

# Methods and Analysis

## Data Cleaning and Pre-Processing
I cleaned the data and partitioned them using source code from the EdX course materials that were provided. This involved installing the required packages, downloading the MovieLens data, generating column names, and partitioning the data into an EdX set (90%) and a Validation hold-out set (10%). I then did some additional data pre-processing, which involved splitting the EdX set itself into a train set (90%) and a test set (10%). I established the RMSE function, which is
$\sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2}$ and which will be used to evaluate the models.

```{r Create the edx and validation sets, include = FALSE}
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(dplyr)
library(lubridate)
library(ggplot2)
library(dslabs)
library(knitr)
library(kableExtra)
library(colorspace)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
head(temp)

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId") 
head(validation)

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

#Because the validation set from above should NOT be used until a final check of the chosen RMSE model, 
#here I split the edx set into a test and train set
options(digits=7)

set.seed(1) 
test_index_edx <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
edx_train_set <- edx[-test_index_edx,]
temp <- edx[test_index_edx,]
edx_test_set <- temp %>% 
  semi_join(edx_train_set, by = "movieId") %>%
  semi_join(edx_train_set, by = "userId") #this step ensures I don’t include users and movies in the test set that do not appear in the training set
removed <- anti_join(temp, edx_test_set)
edx_train_set <- rbind(edx_train_set, removed)

rm(test_index_edx, temp, removed)
edx_train_set
edx_test_set

#Start by writing an RMSE function based on ratings and predictors 
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

## Data Exploration and Visualization
In this section, I will explore the data set and visualize the data. First, I show the head of the datasets generated. Here is the head of the EdX set. 
```{r Head EdX, echo = FALSE}
head(edx, 10) %>% kbl(caption = "Head of EdX Data Set") %>%
  kable_classic(full_width = F, html_font = "Cambria")%>% 
  kable_styling(latex_options = "HOLD_position")
```

And here is the head of the Validation set, which will not be used in the model generation. It will only be used at the end to test the efficacy of the model.
```{r Head validation, echo = FALSE}
head(validation, 10) %>% kbl(caption = "Head of Validation Data Set") %>%
  kable_classic(full_width = F, html_font = "Cambria")%>% 
  kable_styling(latex_options = "HOLD_position")
```

And here is the head of both the EdX train and EdX test sets. 
```{r Head edx train and test, echo = FALSE}
head(edx_train_set, 10) %>% kbl(caption = "Head of EdX Train Data Set") %>%
  kable_classic(full_width = F, html_font = "Cambria")%>% 
  kable_styling(latex_options = "HOLD_position")
head(edx_test_set, 10) %>% kbl(caption = "Head of EdX Test Data Set") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

I then determine the number of rows and columns in the EdX data. Our data set has about 9 million rows, where each row represents one user’s rating of one specific movie 
```{r rows and columns, echo=FALSE}
littletable <- matrix(c(nrow(edx), ncol(edx)), ncol=2, byrow=TRUE)
colnames(littletable) <- c("Number of Rows", "Number of Columns")
littletable %>% kbl(caption = "Number of Rows and Columns in the EdX Data Set") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

Here, I show a plot of the count vs. ratings given. From this graph, it is clear that whole number ratings (1,2,3,4,5) are more common than half-ratings (0.5, 1.5, 2.5, 3.5, 4.5). 
```{r count vs. ratings, fig.cap = "Plot of count of ratings by rating given.", echo = FALSE}
edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count, color = I("aquamarine4"))) +ggtitle("Count vs. Rating Given") + 
  xlab("Rating") + 
  ylab("Count") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  geom_line()
```

Here, I show the number of distinct MovieIds and UserIds in the EdX dataset. If those numbers were multiplied, we would have over 745 million entries, but our data set has only around 9 million entries. Therefore, not every user rated every movie. 
```{r distinct movieIds and userIds, echo = FALSE}
#calculate the number of distinct movieIds and distinct userIds
littletable_distinct <- matrix(c(nrow(as.data.frame(unique(edx$movieId))), nrow(as.data.frame(unique(edx$userId)))))
rownames(littletable_distinct) <- c("Unique MovieId", "Unique UserId")
littletable_distinct %>% kbl(caption = "Number of Distinct movieIds and userIds in the EdX Data Set") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

The above concept can be visualized. Here, I show a plot of a sample of 100 ratings, with orange representing movies a user has rated and white representing movies that haven't been rated. The spaces that aren't filled in with orange have not been rated. We can think of this recommendation system as filling in those spaces. 
```{r Plot of 100 Ratings, fig.cap = "Plot of users vs. movies for a sample of 100 random ratings.", echo=FALSE}
users <- sample(unique(edx$userId), 100)
edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. ,col = hcl.colors(12, "TealGrn", rev = TRUE), xlab="Movies", ylab="Users", main = "Users vs. Movies Sample of 100")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```

## Non-Regularization Bias Models

In this section, I show the models I developed for the recommendation system. I begin by developing the simplest possible recommendation system, which predicts the same rating for all movies regardless of user. I call this model naive_rmse. This model is based on the following function, $Y_{u,i}=\mu + \varepsilon_{u,i}$, where $\mu$ is the "true" rating for the movies and $\varepsilon_{i,u}$ are independent errors are sampled from the same distribution centered at 0.

```{r simplest rec system, echo = FALSE}
options(pillar.sigfig = 7)

#Simplest possible recommendation system
#the estimate minimizing RMSE is the least squares estimate (avg of all ratings)
mu_hat <- mean(edx_train_set$rating)

#predict all unknown ratings with mu_hat gives the naive_rmse
naive_rmse <- RMSE(edx_test_set$rating, mu_hat)

#results table of the naive approach 
rmse_results <- tibble(method = "Avg Naive Model", RMSE = naive_rmse)
rmse_results %>% kbl(caption = "Initial RMSE Results - Average Naive Model") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

The naive model has an RMSE of 1.059. That is our baseline on which to improve. Next, I create a movie rating system using the movieID, with the idea that some movies are generally rated higher than others. The model follows $Y_{u,i}=\mu + b_i + \varepsilon_{u,i}$, where $b_i$ is the bias by movie. Because of the size of the data set, I will estimate $b_i$ using the least squares estimate $\hat{b}_i$. Here, one can see the RMSE for the average movie rating model. 

```{r average movie rating model, echo=FALSE}
#each movie has a different avg rating, b_i, the bias by movie. Because of the size of the data set 
#we will estimate b_i using the least squares estimate b_i_hat
mu <- mean(edx_train_set$rating) 
movie_avgs <- edx_train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i_hat = mean(rating - mu))

#show improved RMSE using mu + b_i_hat
predicted_ratings <- mu + edx_test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i_hat)
avg_movie_rmse <- RMSE(predicted_ratings, edx_test_set$rating)

#updating rmse_results
rmse_results <- rmse_results %>% add_row(method = "Avg Movie Rating Model", RMSE = avg_movie_rmse)
rmse_results %>% kbl(caption = "RMSE Results - Including Average Movie Rating Model") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

The movie rating model has an RMSE of 0.9427. This is a significant improvement. Next, I will factor in users to attempt to further improve RMSE. Some users may generally rate movies lower, while some may rate them higher -- even in the case that a movie is generally highly rated. This complicates the movieID recommendation system by coupling it with information on a given user. I begin by computing average ratings for user u and visualizing that graphically. This is modeled by $Y_{u,i}=\mu + b_i + b_u + \varepsilon_{u,i}$, where $b_u$ is the bias by user. Here, again, Because of the size of the data set, I will estimate $b_u$ using the least squares estimate $\hat{b}_u$. I calculate the RMSE for the average movie and user rating model, comparing it to the average movie rating model. 

```{r average movie and user rating model, echo=FALSE}
#factor in user averages to improve RMSE 
  
#each user has a different avg rating, b_u, the bias by user. Because of the size of the data set 
#we will estimate b_u using the least squares estimate b_u_hat
user_avgs <- edx_train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u_hat = mean(rating - mu - b_i_hat))

#check RMSE 
predicted_ratings <- edx_test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i_hat + b_u_hat) %>%
  pull(pred)
avg_movie_user_rmse<- RMSE(edx_test_set$rating, predicted_ratings)

#updating rmse_results
rmse_results <- rmse_results %>% add_row(method = "Avg Movie + User Rating Model", RMSE = avg_movie_user_rmse)
rmse_results %>%  kbl(caption = "RMSE Results - Including Average Movie and User Rating Model") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

I then show a plot of the variability in estimates that come from b_u_hat, which shows that user ratings vary by user -- some generally rate movies highly, others do not. 
```{r bar graph, fig.cap = "Plot of the average rating for user u.", echo = FALSE}
#start by computing average rating for user u and visualizing it graphically
edx_train_set %>% 
  group_by(userId) %>% 
  filter(n()>=100) %>%
  summarize(b_u_hat = mean(rating)) %>% 
  ggplot(aes(b_u_hat)) + ggtitle("Average Rating for User U") +
  geom_histogram(bins = 30, color = I("aquamarine4"), fill = I("aquamarine3")) +
  xlab("b_u_hat") + 
  ylab("Count") +
  theme(plot.title = element_text(hjust = 0.5))
```

Incorporating user, we once again see a large improvement with a RMSE of 0.8646. This is already below the project threshold value of 0.8649, but let's see if we can improve upon it. I will now incorporate biases other than movieID and user, namely the impact that genre has on the recommendation system. This is modeled by $Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}$, with $x^k_{u,i} = 1$ if $g_{u,i}$ is genre $k$. $b_g$ is the bias by cross-listed genre and $k$ is each given genre. Here, I use cross-listed genres as they appear in the data set. For example, Action|Adventure is treated as one genre. Because of the size of the data set, I will estimate $b_g$ using the least squares estimate $\hat{b}_g$. I start by visualizing the average rating for each cross-listed genre g graphically. Like with users, this graph shows us that different genres generally receive different ratings. 

```{r fig of genres, fig.cap = "Plot of the average rating for cross-listed genres g.", echo = FALSE}
#start by computing average rating for user g and visualizing it graphically
edx_train_set %>% 
  group_by(genres) %>% 
  filter(n()>=100) %>% 
  summarize(b_g_hat = mean(rating)) %>% 
  ggplot(aes(b_g_hat)) + ggtitle("Average Rating for Cross-Listed Genres G") +
  geom_histogram(bins = 30, color = I("aquamarine4"), fill = I("aquamarine3")) +
  xlab("b_g_hat") + 
  ylab("Count") +
  theme(plot.title = element_text(hjust = 0.5))
```

I calculate the RMSE for the average movie and user and cross-listed genre rating model, comparing it to the prior two models. 
```{r average movie and user and cross-listed genre rating model, echo=FALSE}
#each genre combo has a different avg rating, b_g, the bias by genre Because of the size of the data set 
#we will estimate b_g using the least squares estimate b_g_hat
genres_avgs <- edx_train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g_hat = mean(rating - mu - b_i_hat - b_u_hat))

#check RMSE 
predicted_ratings <- edx_test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  mutate(pred = mu + b_i_hat + b_u_hat + b_g_hat) %>%
  pull(pred)
avg_movie_user_genres_rmse<- RMSE(edx_test_set$rating, predicted_ratings)

#updating rmse_results
rmse_results <- rmse_results %>% add_row(method = "Avg Movie + User + Genres Rating Model", RMSE = avg_movie_user_genres_rmse)
rmse_results %>%  kbl(caption = "RMSE Results - Including Average Movie and User and Cross-Listed Genres Rating Model") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

With the cross-listed genres added to the model, we get an RMSE of 0.8643, which is a small improvement from the movie and user model. Now I consider genres separated out from their cross-listed format. For example, Action|Adventure is separated into Action and Adventure. This is modeled by the same equation: $Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}$, with $x^k_{u,i} = 1$ if $g_{u,i}$ is genre $k$. $b_sg$ is the bias by separate genre. Because of the size of the data set, I will estimate $b_sg$ using the least squares estimate $\hat{b}_sg$. I start by separating the data set by separate, non-cross-listed, genres. Once I do so, the head of the data set looks like the following. 

```{r separate genres, echo=FALSE}
sep_by_genre <- edx %>% separate_rows(genres, sep = "\\|")
genres <- sep_by_genre %>% group_by(genres)

sep_by_genre_train <- edx_train_set %>% separate_rows(genres, sep = "\\|")
sep_by_genre_test <- edx_test_set %>% separate_rows(genres, sep = "\\|")

grouped_sep_train <- sep_by_genre_train %>% group_by(genres)
grouped_sep_test <- sep_by_genre_test %>% group_by(genres)

head(genres) %>%  kbl(caption = "Head of Separated Genres") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

Here, you can see the count of ratings for each separated genre in the train set. 
```{r train set number, echo = FALSE}
grouped_sep_train %>% 
  summarize(count = n()) %>%
  arrange(desc(count))  %>%  kbl(caption = "Separated Genres Count of Ratings -- Train Set") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

And here is the count of ratings for each separated genre in the test set. 
```{r test set number, echo = FALSE}
grouped_sep_test %>% 
  summarize(count = n()) %>%
  arrange(desc(count)) %>%  kbl(caption = "Separated Genres Count of Ratings -- Test Set") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

I then visualize the average rating for each separate genre graphically, which shows us that different separated genres also generally receive different ratings. 
```{r plot sep genres, fig.cap = "Plot of the average rating for separate genres.", echo = FALSE}
#start by computing average rating for user g and visualizing it graphically
genres %>% 
  summarize(b_g_hat = mean(rating)) %>% 
  ggplot(aes(genres, b_g_hat, color = genres)) + geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Average Rating for Separated Genres - EdX Set") +
  xlab("Genres") + 
  ylab("b_g_hat") +
  theme(plot.title = element_text(hjust = 0.5))
```

Here, I calculate the RMSE for the average movie and user and separated genre rating model, comparing it to the prior two models. 
```{r average movie and user and separated genre rating model, echo = FALSE}
#each genre combo has a different avg rating, b_g, the bias by genre Because of the size of the data set 
#we will estimate b_g using the least squares estimate b_g_hat
genres_avgs <- sep_by_genre_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g_hat = mean(rating - mu - b_i_hat - b_u_hat))

#check RMSE 
predicted_ratings <- sep_by_genre_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  mutate(pred = mu + b_i_hat + b_u_hat + b_g_hat) %>%
  pull(pred)
avg_movie_user_sepgenres_rmse<- RMSE(sep_by_genre_test$rating, predicted_ratings)

#updating rmse_results
rmse_results <- rmse_results %>% add_row(method = "Avg Movie + User + Separated Genres Rating Model", RMSE = avg_movie_user_sepgenres_rmse)
rmse_results %>%  kbl(caption = "RMSE Results - Including Average Movie and User and Separated Genres Rating Model") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

The RMSE for the movie, user, and separated genres model is 0.8626, which is the lowest RMSE obtained thus far. I will now see if improvement upon this model can be made using regularization. 

## Regularized Bias Models

To understand the reasoning behind the regularization process, I first show the 10 best movies without regularization. These films are all fairly obscure, which doesn't make much sense. 
```{r 10 best movies, echo = FALSE}
#finding distinct titles of movies in the data set 
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()

#10 best movies from my estimate
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i_hat)) %>% 
  slice(1:10)  %>% 
  pull(title) %>% kbl(caption = "10 Best Movies Without Regularization") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

Here are the 10 worst movies without regularization, which are also quite obscure. 
```{r 10 worst movies, echo = FALSE}
#10 worst movies from my estimate
movie_avgs %>% left_join(movie_titles, by="movieId") %>%
  arrange(b_i_hat) %>% 
  slice(1:10)  %>% 
  pull(title) %>% kbl(caption = "10 Worst Movies Without Regularization") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

However, this obscurity can be explained when we look at the number of times these movies were rated. We can see that the top 10 best movies and the top 10 worst movies are generally rated very few times, because the low number of ratings warps their ranking among movies. 
```{r number of ratings, echo = FALSE}
#how often are the 10 best movies rated? 
edx_train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(b_i_hat)) %>% 
  slice(1:10) %>%
  select(title,n) %>% kbl(caption = "Number of Ratings for the 10 Best Movies") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")

#how often are the 10 worst movies rated? 
edx_train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by = "movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i_hat) %>% 
  slice(1:10) %>% 
  select(title,n) %>% kbl(caption = "Number of Ratings for the 10 Worst Movies") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

As you can see, the 10 best and worst movies without regularization are commonly rated by only one user. Because the number of users rating a given movie is so low in these cases, a given rating can elevate or tank that movie's rating. Regularization seeks to remedy that problem. I first begin by regularizing the movie rating model. Using cross-validation, I pick the optimal lambda which is, as you can see graphically, 1.5. 

```{r movie rating lambda, fig.cap = "Plot of lambdas for the regularized movie method.", echo = FALSE}
#choose the optimal lambda by cross-validation for movie method
lambdas <- seq(0, 10, 0.25)

mu <- mean(edx_train_set$rating)
just_the_sum <- edx_train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- edx_test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i_hat = s/(n_i+l)) %>%
    mutate(pred = mu + b_i_hat) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_set$rating))
})
qplot(lambdas, rmses,  color = I("aquamarine4")) + ggtitle("Lambdas for Regularized Movie Model") +
  xlab("Lambdas") + 
  ylab("RMSEs") + 
  theme(plot.title = element_text(hjust = 0.5))
lambda <- lambdas[which.min(rmses)]
```

Given the optimal lambda, I then input that value to calculate the regularized movie rating RMSE. 
```{r regularized movie rating, echo = FALSE}
mu <- mean(edx_train_set$rating)
regularized_movie <- edx_train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i_hat = sum(rating - mu)/(n()+lambda), n_i = n()) 

predicted_ratings <- edx_test_set %>% 
  left_join(regularized_movie, by = "movieId") %>%
  mutate(pred = mu + b_i_hat) %>%
  pull(pred)

regularized_movie_rmse <- RMSE(predicted_ratings, edx_test_set$rating)

#updating rmse_results
rmse_results <- rmse_results %>% add_row(method = "Regularized Movie Rating Model", RMSE = regularized_movie_rmse)
rmse_results %>%  kbl(caption = "RMSE Results - Including Regularized Movie Rating Model") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

The RMSE of the regularized movie rating model is 0.9426, which is almost identical to the un-regularized model. The two differ only at the fifth significant figure. The contrast between the regularized method is visible by looking again at the top 10 movie estimates that we previously examined, but this time using regularization.  
```{r regularized proof, echo = FALSE}
#top 10 movie estimates with regularization 
edx_train_set %>%
  count(movieId) %>% 
  left_join(regularized_movie, by = "movieId") %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i_hat)) %>% 
  slice(1:10) %>% 
  pull(title) %>% kbl(caption = "10 Best Movies With Regularization") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

And here are the 10 worst movies with regularization.
```{r regularized proof 2, echo = FALSE}
#bottom 10 movie estimates with regularization 
edx_train_set %>%
  count(movieId) %>% 
  left_join(regularized_movie, by = "movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(b_i_hat) %>% 
  select(title, b_i_hat, n) %>% 
  slice(1:10) %>% 
  pull(title) %>% kbl(caption = "10 Worst Movies With Regularization") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

These films make much more sense as the 10 best and worst rated movies. Here, I plot of the regularized estimates versus the least squares estimates to see how the estimates change with regularization.
```{r reg vs. original, fig.cap = "Regularized vs. Original plot using square root of n", echo = FALSE}
tibble(original = movie_avgs$b_i_hat, 
       regularlized = regularized_movie$b_i_hat, 
       n = regularized_movie$n_i) %>%
  ggplot(aes(original, regularlized, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5) + ggtitle("Regularized vs. Original Least Squares Estimates") +
  xlab("Original") + 
  ylab("Regularized") +
  theme(plot.title = element_text(hjust = 0.5))
```

The same process is then repeated for the movie and user rating model to choose the cross-validated, optimal lambda, which is 5. I then input the minimum lambda to calculate the regularized RMSE.  
```{r regularized movie and user rating plot, fig.cap = "Plot of lambdas for the regularized movie and user method.", echo = FALSE}
#choose the optimal lambda by cross-validation for movie and user method
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(edx_train_set$rating)
  
  b_i_hat <- edx_train_set %>% 
    group_by(movieId) %>%
    summarize(b_i_hat = sum(rating - mu)/(n()+l))
  
  b_u_hat <- edx_train_set %>% 
    left_join(b_i_hat, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u_hat = sum(rating - b_i_hat - mu)/(n()+l))
  
  predicted_ratings <- 
    edx_test_set %>% 
    left_join(b_i_hat, by = "movieId") %>%
    left_join(b_u_hat, by = "userId") %>%
    mutate(pred = mu + b_i_hat + b_u_hat) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, edx_test_set$rating))
})
qplot(lambdas, rmses, color = I("aquamarine4")) + ggtitle("Lambdas for Regularized Movie and User Model") +
  xlab("Lambdas") + 
  ylab("RMSEs") +
  theme(plot.title = element_text(hjust = 0.5))
lambda <- lambdas[which.min(rmses)]
```

```{r regularized movie and user rating, echo = FALSE}
#inputting minimum lamda to calculate rmse
mu <- mean(edx_train_set$rating)

b_i_hat <- edx_train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i_hat = sum(rating - mu)/(n()+lambda), n_i = n()) 

b_u_hat <- edx_train_set %>% 
  left_join(b_i_hat, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u_hat = sum(rating - b_i_hat - mu)/(n()+lambda), n_i = n())

predicted_ratings <- 
  edx_test_set %>% 
  left_join(b_i_hat, by = "movieId") %>%
  left_join(b_u_hat, by = "userId") %>%
  mutate(pred = mu + b_i_hat + b_u_hat) %>%
  pull(pred)

regularized_movie_user_rmse <- RMSE(predicted_ratings, edx_test_set$rating)

options(pillar.sigfig = 7)

#updating rmse_results
rmse_results <- rmse_results %>% add_row(method = "Regularized Movie + User Rating Model", RMSE = regularized_movie_user_rmse)
rmse_results %>%  kbl(caption = "RMSE Results - Including Regularized Movie and User Rating Model") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

The RMSE of the regularized movie and user model is 0.8640, which is a slight improvement upon the movie and user rating model, but still performs worse than both models that include genre as an additional bias. 

# Results 
The results of this analysis show that the most effective model is the average movie, user, and separated genres rating model. The results further imply that the more biases included, the more effective the model is. I will now proceed to test the most successful model with the original EdX and Validation sets to see if the RMSE remains below the project threshold RMSE of 0.8649. Here, as with before, I will separate genre instead of using the combination of the cross-listed genres, this time with the EdX and Validation sets. For example, Action|Adventure is separated into Action and Adventure. I then look at Action as a separate category from Adventure.

Here, I show the ratings, grouped by separated genre, for the EdX set and the Validation set.
```{r set up the seperated by genre sets, fig.pos = 'H', echo = FALSE}
#Because the Avg Movie + User + Separated Genres Rating Model had the lowest RMSE of 0.8627, I will now use that model with the validation set. 

sep_by_genre_edx <- edx %>% separate_rows(genres, sep = "\\|")
sep_by_genre_validation <- validation %>% separate_rows(genres, sep = "\\|")
genres_val <- sep_by_genre_validation %>% group_by(genres)

grouped_sep_edx <- sep_by_genre_edx %>% group_by(genres)
grouped_sep_validation <- sep_by_genre_validation %>% group_by(genres)

grouped_sep_edx %>% 
  summarize(count = n()) %>%
  arrange(desc(count)) %>%  kbl(caption = "Ratings for EdX Set Grouped by Genre") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling()
```

```{r Plot of ratings for validation set, fig.pos = 'H', echo = FALSE}
grouped_sep_validation %>% 
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  arrange(desc(count)) %>%  kbl(caption = "Ratings for Validation Set Grouped by Genre") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling()
```

I start by computing average ratings for each separated genres for the Validation set and visualizing them graphically. We can again see that different separated genres generally receive different ratings. Next, I will calculate the average for each individual genre, k. Each genre combo has a different avg rating, b_g, the bias by genre. Because of the size of the data set. We will estimate b_g using the least squares estimate b_g_hat. 
```{r , fig.cap = "Plot of the average rating for separate genres for the validation set.", fig.pos = 'H', echo = FALSE}
#start by computing average rating for each separated genres and visualizing them graphically
genres_val %>% 
  summarize(b_g_hat = mean(rating)) %>% 
  ggplot(aes(genres, b_g_hat, color = genres)) + geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Average Rating for Separate Genres - Validation Set") + xlab("Lambdas") + 
  ylab("RMSEs") +
  theme(plot.title = element_text(hjust = 0.5))
```

I then calculate the final RMSE using the Validation set for the movie, user, and separated genre model. 
```{r, echo = FALSE}
#I will calculate the average for each individual genre, k.
#each genre combo has a different avg rating, b_g, the bias by genre. Because of the size of the data set 
#we will estimate b_g using the least squares estimate b_g_hat
genres_avgs_final <- sep_by_genre_edx %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g_hat = mean(rating - mu - b_i_hat - b_u_hat))

#check RMSE 
predicted_ratings <- sep_by_genre_validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genres_avgs, by='genres') %>%
  mutate(pred = mu + b_i_hat + b_u_hat + b_g_hat) %>%
  pull(pred)
final_validation_avg_movie_user_sepgenres_rmse<- RMSE(sep_by_genre_validation$rating, predicted_ratings)

#updating rmse_results with Final Validation Set
rmse_results <- rmse_results %>% add_row(method = "Final Validation Avg Movie + User + Separated Genres Rating Model", RMSE = final_validation_avg_movie_user_sepgenres_rmse)
rmse_results %>% kbl(caption = "RMSE Results - Final Validation Movie and User and Separated Genres Rating Model") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  kable_styling(latex_options = "HOLD_position")
```

As is visible from the above table, the RMSE of the final Validation and EdX set is 0.8639. This value is higher than that obtained using the EdX train and test set, 0.8626, but is still well below the threshold RMSE of 0.8649.

# Conclusion
As part of the final course for the Harvard EdX professional certificate in data science, I set out to develop a model to make movie recommendations that would have an RMSE below 0.8649. I began by pre-processing and cleaning the data, making sure to split the EdX set into a train and test set, while saving the Validation set as the final hold-out set. I then visualized the data, before beginning to test a series of models. I found that the more biases applied to a model, the better the model performs. For instance, the movies model performed worse than the movies and users model, which performed worse than the movies, users, and genres model. While regularized models performed very slightly better than their non-regularized counterparts, the difference between each corresponding pair was extremely small (variation began only at the fifth significant figure). The most successful model I tested was the movies, users, and separated genres model, which had a final RMSE of 0.8639 using the final hold-out Validation set. This is below the target RMSE of 0.8649 and is therefore a successful model that achieves the goal of this project.  
	
While this model performed well, there is much future work that could be done to expand and improve this report. Further biases, such as time of rating, could be used to develop models with more biases. Given my results, models with more biases included would likely have lower RMSE values. In addition, more models could be tested with regularization. Because of the processing time regularization requires, I was only able to regularize the movie model and the movie and user model. In the future, I would be interested to test regularization on each model. Use of other techniques, such as matrix factorization, RandomForest, and SVD could also be interesting expansions of this study. 

# Sources: 
* Chen, E. (2011, October 24). Winning the Netflix Prize: A Summary. http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/.
* Irizarry, R. A. (2020, March 2). Introduction to Data Science. Retrieved from https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#notation-1. 
* Koren, Y. (2009). The bellkor solution to the netflix grand prize. Netflix prize documentation, 81, 1-10. https://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf.
* Lohr, S. (2009, September 21). Netflix Awards $1 Million Prize and Starts a New Contest. https://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/.
